As Product Owner, my main wish is to create a highly automated, robust, and future-proof system that makes it easy for me—and eventually other users—to run TinyLlama inference on AWS with minimal manual effort and maximum transparency. I want a clean, user-friendly desktop GUI where I can write prompts, see responses, manage costs, and control the GPU instance lifecycle (start, stop, idle-timeout) in real time, all from my own computer. I do not want to deal with personalized model training or fine-tuning yet; instead, I want to use a pre-trained, open-source TinyLlama model that produces coherent answers out of the box, with the architecture designed so future upgrades (like personalized LoRA adapters and multi-user support) can be added easily. The entire backend should be automated and secure, using industry best practices for CI/CD, AMI builds, cost management, and disaster recovery, so that any code or configuration change in GitHub results in a new AMI and a seamless deployment. I want the system to be both cost-effective and reliable, with clear safeguards to avoid AWS surprises (like unexpected costs or service limitations). Most importantly, I expect all solutions to be practical, fully documented, and actionable, so that even if another team picks up the requirements, they can build and maintain the project without extra context or prior knowledge.